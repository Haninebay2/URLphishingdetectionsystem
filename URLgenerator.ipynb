{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_yGl8aWmbe5",
        "outputId": "21550def-6be8-489d-dbf6-8605537f0053"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: faker in /usr/local/lib/python3.10/dist-packages (24.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
            "Requirement already satisfied: dnspython in /usr/local/lib/python3.10/dist-packages (2.6.1)\n",
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.10/dist-packages (5.1.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from tldextract) (3.7)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tldextract) (2.31.0)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.10/dist-packages (from tldextract) (2.0.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract) (3.13.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract) (2024.2.2)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
            "Requirement already satisfied: whois in /usr/local/lib/python3.10/dist-packages (1.20240129.2)\n",
            "Requirement already satisfied: python-whois in /usr/local/lib/python3.10/dist-packages (0.9.4)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from python-whois) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->python-whois) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install faker\n",
        "!pip install dnspython\n",
        "!pip install tldextract\n",
        "!pip install validators\n",
        "!pip install whois\n",
        "!pip install python-whois"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "PuxZ14rID94q",
        "outputId": "9bf58a0e-e0a3-4130-c59b-d119465f62c6"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a477f27b33ad>\u001b[0m in \u001b[0;36m<cell line: 293>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;31m# Example usage with API keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_and_collect_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_suspicious\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcse_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcse_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarweb_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msimilarweb_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_browsing_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_browsing_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muuid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muuid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'url_data.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-a477f27b33ad>\u001b[0m in \u001b[0;36mgenerate_and_collect_data\u001b[0;34m(count, include_suspicious, suspicious_ratio, api_key, cse_id, similarweb_key, safe_browsing_key, uuid)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \"\"\"\n\u001b[1;32m    259\u001b[0m     \u001b[0;31m# Generate URLs with associated labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0murls_with_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_random_urls_optimized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-a477f27b33ad>\u001b[0m in \u001b[0;36mgenerate_random_urls_optimized\u001b[0;34m(count)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphishing_urls_set\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mdomain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphishing_domains\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0mdeceptive_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeceptive_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/random.py\u001b[0m in \u001b[0;36mchoice\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[0;31m## -------------------- sequence methods  -------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m         \u001b[0;34m\"\"\"Choose a random element from a non-empty sequence.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;31m# raises IndexError if seq is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import urllib.request\n",
        "import random\n",
        "from faker import Faker\n",
        "import requests\n",
        "import dns.resolver\n",
        "import tldextract\n",
        "import ipaddress\n",
        "import json\n",
        "import requests\n",
        "from urllib.parse import urlencode\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from google.colab import userdata\n",
        "import re\n",
        "from itertools import product\n",
        "import validators\n",
        "api_key = userdata.get('APIKEY')\n",
        "cse_id = userdata.get('cseid')\n",
        "similarweb_key = userdata.get('similarkey')\n",
        "safe_browsing_key =  userdata.get('Safebrowsing')\n",
        "uuid = userdata.get('urlscanapi')\n",
        "\n",
        "fake = Faker()\n",
        "user_agent = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.116 Safari/537.36'\n",
        "\n",
        "import whois\n",
        "\n",
        "def check_whois_registered(domain):\n",
        "    try:\n",
        "        w = whois.whois(domain)\n",
        "        # Instead of checking just domain_name, check for other fields as well\n",
        "        if any([w.domain_name, w.registrar, w.creation_date, w.expiration_date]):\n",
        "            return 1\n",
        "    except whois.parser.PywhoisError as e:  # Adjusted for a more specific exception\n",
        "        print(f\"WHOIS lookup failed for {domain}: {e}\")\n",
        "    except Exception as e:  # Catching any other exception that might occur\n",
        "        print(f\"Unexpected error during WHOIS lookup for {domain}: {e}\")\n",
        "    return 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def generate_random_urls_optimized(count=15000):\n",
        "    fake = Faker()\n",
        "\n",
        "    real_domains = [\n",
        "        \"google.com\", \"facebook.com\", \"amazon.com\", \"youtube.com\", \"wikipedia.org\",\n",
        "        \"twitter.com\", \"linkedin.com\", \"instagram.com\", \"reddit.com\", \"pinterest.com\"\n",
        "    ]\n",
        "\n",
        "    phishing_domains = [\n",
        "        \"google-security.com\", \"facebook-login.com\", \"amaz0n.co\", \"y0utube.com\", \"wikipediia.org\",\n",
        "        \"tw1tter.com\", \"linkedinn.biz\", \"instagrarn.com\", \"redd1t.net\", \"pint3rest.com\"\n",
        "    ]\n",
        "\n",
        "    deceptive_paths = ['/login/', '/verify/', '/secure/', '/update-account/', '/confirm/']\n",
        "    queries = ['?username=admin&password=admin', '?user=guest&auth=1234', '?account=update&service=mail']\n",
        "\n",
        "    real_paths = set()\n",
        "    phishing_urls_set = set()\n",
        "\n",
        "    # Generate unique real paths\n",
        "    while len(real_paths) < count // 2:\n",
        "        path = '/' + '/'.join(fake.words(nb=random.randint(1, 3)))\n",
        "        real_paths.add(path)\n",
        "\n",
        "    # Generate unique phishing URLs\n",
        "    while len(phishing_urls_set) < count // 2:\n",
        "        protocol = 'https://'\n",
        "        domain = random.choice(phishing_domains)\n",
        "        deceptive_path = random.choice(deceptive_paths)\n",
        "        query = random.choice(queries)\n",
        "        phishing_url = protocol + domain + deceptive_path + query\n",
        "        phishing_urls_set.add((phishing_url, 'phishing'))\n",
        "\n",
        "    # Convert sets to lists\n",
        "    real_urls = [(f\"https://{random.choice(real_domains)}{path}\", 'real') for path in real_paths]\n",
        "    phishing_urls = list(phishing_urls_set)\n",
        "\n",
        "    # Shuffle both lists\n",
        "    random.shuffle(real_urls)\n",
        "    random.shuffle(phishing_urls)\n",
        "\n",
        "    # Combine the two lists\n",
        "    urls = real_urls + phishing_urls\n",
        "\n",
        "    # Shuffle the combined list\n",
        "    random.shuffle(urls)\n",
        "\n",
        "    # Ensure the list has the correct count\n",
        "    urls = urls[:count]\n",
        "\n",
        "    # Ensure all URLs are unique and balance is maintained\n",
        "    if len(urls) != len(set(urls)):\n",
        "        raise ValueError(\"Duplicate URLs found in the generated list.\")\n",
        "\n",
        "\n",
        "    return urls\n",
        "\n",
        "\n",
        "\n",
        "import socket\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "def contains_ip(url):\n",
        "    try:\n",
        "        # Extract hostname from URL\n",
        "        hostname = urlparse(url).hostname\n",
        "\n",
        "        # Get the IP address for the hostname\n",
        "        ip_address = socket.gethostbyname(hostname)\n",
        "\n",
        "        return True\n",
        "    except socket.gaierror:\n",
        "        return False\n",
        "\n",
        "def extract_features(url):\n",
        "    suspicious_tlds = ['xyz', 'info', 'top', 'gq', 'cf', 'tk', 'ml', 'ga', 'men', 'loan', 'date', 'win', 'faith', 'review', 'party', 'webcam', 'trade', 'accountant', 'download', 'racing', 'science', 'cricket', 'bid']\n",
        "    features = {}\n",
        "    parsed_url = tldextract.extract(url)\n",
        "    features['length'] = len(url)\n",
        "    features['protocol'] = url.split('://')[0]\n",
        "    features['domain'] = parsed_url.domain\n",
        "    features['subdomain'] = parsed_url.subdomain\n",
        "    features['suffix'] = parsed_url.suffix\n",
        "    features['path'] = url.split(parsed_url.domain)[-1]\n",
        "    features['number_of_subdomains'] = len(parsed_url.subdomain.split('.')) if parsed_url.subdomain else 0\n",
        "    if contains_ip(url):\n",
        "      features['has_ip_address'] = True\n",
        "    else:\n",
        "      features['has_ip_address'] = False\n",
        "    features['is_https'] = features['protocol'] == 'https'\n",
        "    features['special_char_count'] = sum(not c.isalnum() for c in url)\n",
        "    suspicious_words = ['login', 'verify', 'account', 'secure', 'update', 'banking']\n",
        "    features['has_suspicious_word'] = any(word in url for word in suspicious_words)\n",
        "    features['is_suspicious_tld'] = features['suffix'] in suspicious_tlds\n",
        "    return features\n",
        "\n",
        "def get_dns_records(domain):\n",
        "    records = {'A': [], 'MX': [], 'NS': []}\n",
        "    try:\n",
        "        a_records = dns.resolver.resolve(domain, 'A')\n",
        "        records['A'] = [rdata.address for rdata in a_records]\n",
        "    except (dns.resolver.NXDOMAIN, dns.resolver.NoAnswer, dns.resolver.Timeout, dns.exception.DNSException):\n",
        "        records['A'] = 'Unavailable'\n",
        "    try:\n",
        "        mx_records = dns.resolver.resolve(domain, 'MX')\n",
        "        records['MX'] = [rdata.exchange.to_text() for rdata in mx_records]\n",
        "    except (dns.resolver.NXDOMAIN, dns.resolver.NoAnswer, dns.resolver.Timeout, dns.exception.DNSException):\n",
        "        records['MX'] = 'Unavailable'\n",
        "    try:\n",
        "        ns_records = dns.resolver.resolve(domain, 'NS')\n",
        "        records['NS'] = [rdata.to_text() for rdata in ns_records]\n",
        "    except (dns.resolver.NXDOMAIN, dns.resolver.NoAnswer, dns.resolver.Timeout, dns.exception.DNSException):\n",
        "        records['NS'] = 'Unavailable'\n",
        "    return records\n",
        "\n",
        "\n",
        "\n",
        "# import requests\n",
        "# import os\n",
        "# import time\n",
        "# from bs4 import BeautifulSoup\n",
        "# from urllib.parse import urlencode\n",
        "\n",
        "# def check_google_indexing(url, user_agent):\n",
        "#     headers = {'User-Agent': user_agent}\n",
        "#     query = {'q': 'info:' + url}\n",
        "#     google_url = \"https://www.google.com/search?\" + urlencode(query)\n",
        "\n",
        "#     try:\n",
        "#         response = requests.get(google_url, headers=headers)\n",
        "#         response.raise_for_status()\n",
        "#         soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "#         # Check if the URL is indexed\n",
        "#         check = soup.find(id=\"rso\").find(\"div\", recursive=False).find(\"div\", recursive=False).find(\"h3\").find(\"a\")\n",
        "#         indexed = True if check else False\n",
        "#         print(f\"{url} is indexed: {indexed}\")\n",
        "#         return indexed\n",
        "#     except requests.exceptions.HTTPError as e:\n",
        "#         print(f\"HTTP error occurred: {e}\")\n",
        "#         return False\n",
        "#     except requests.exceptions.RequestException as e:\n",
        "#         print(f\"Other request error occurred: {e}\")\n",
        "#         return False\n",
        "\n",
        "#     return indexed\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import tldextract\n",
        "\n",
        "def generate_and_collect_data(count=15000, include_suspicious=False, suspicious_ratio=0.3, api_key=None, cse_id=None, similarweb_key=None, safe_browsing_key=None, uuid=None):\n",
        "    \"\"\"\n",
        "    Generate a dataset of URLs with associated features, labels, and WHOIS registration status.\n",
        "\n",
        "    Parameters:\n",
        "    - count (int): The number of URLs to generate.\n",
        "    - include_suspicious (bool): Flag to include suspicious URLs.\n",
        "    - suspicious_ratio (float): The proportion of suspicious URLs if included.\n",
        "    - api_key, cse_id, similarweb_key, safe_browsing_key, uuid: API keys and IDs for various services.\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame: A DataFrame containing the URL data with features, labels, and WHOIS registration status.\n",
        "    \"\"\"\n",
        "    # Generate URLs with associated labels\n",
        "    urls_with_labels = generate_random_urls_optimized(count)\n",
        "\n",
        "    data = []\n",
        "    for url, label in urls_with_labels:\n",
        "        # Initialize the data dictionary for the URL\n",
        "        url_data = {'URL': url, 'Label': label}\n",
        "\n",
        "        # Extract URL features\n",
        "        url_data.update(extract_features(url))\n",
        "\n",
        "        # Parse the domain information\n",
        "        parsed_url = tldextract.extract(url)\n",
        "        full_domain = f\"{parsed_url.subdomain}.{parsed_url.domain}.{parsed_url.suffix}\".strip('.')\n",
        "\n",
        "        # Add DNS records if the domain is valid\n",
        "        if full_domain:\n",
        "            url_data.update(get_dns_records(full_domain))\n",
        "\n",
        "        # Check WHOIS registration status\n",
        "        url_data['WHOIS_Registered'] = check_whois_registered(full_domain)\n",
        "\n",
        "        # Additional API-based features can be added here\n",
        "\n",
        "        # Append the data dictionary to the list\n",
        "        data.append(url_data)\n",
        "\n",
        "    # Convert the list of dictionaries into a DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "# Example usage with API keys\n",
        "df = generate_and_collect_data(1500, include_suspicious=True, api_key=api_key, cse_id=cse_id, similarweb_key=similarweb_key, safe_browsing_key = safe_browsing_key, uuid=uuid)\n",
        "df.to_csv('url_data.csv', index=False)\n",
        "print(df)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
